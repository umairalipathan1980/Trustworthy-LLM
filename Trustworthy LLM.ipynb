{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e463779e-b95f-4629-8e60-9a1583683b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.cleanlab import CleanlabTLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56650317-6727-41b5-99f2-244db4d35aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"model\": \"claude-3.5-sonnet-v2\",\n",
    "    \"max_tokens\": 128,\n",
    "    \"log\": [\"explanation\"]\n",
    "}\n",
    "llm = CleanlabTLM(api_key=\"226eeab91e944b23bd817a46dbe3c8ae\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b21cb58d-7b39-4510-99d2-b85ef52e0428",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.complete(\"which one of 9.11 and 9.9 is greater?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45a01966-be44-435b-b429-2848c7b63026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.11 is greater than 9.9\n",
      "\n",
      "To compare decimal numbers, we can look at each decimal place from left to right. In this case:\n",
      "- Both numbers have the same whole number (9)\n",
      "- Looking at the tenths place (first decimal place):\n",
      "  * 9.11 has 1 tenth\n",
      "  * 9.9 has 9 tenths\n",
      "  * 9.9 can also be written as 9.90\n",
      "- Looking at the hundredths place (second decimal place):\n",
      "  * 9.11 has 1\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f709e9e-177c-4d61-a77d-04445a856e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer starts correctly but is incomplete and has a major error. It states 9.11 is greater but then shows 9.9 has 9 tenths while 9.11 has 1 tenth, which would actually make 9.9 greater. The explanation contradicts itself and cuts off mid-sentence. A nswer would explain that 9.9 = 9.90, and 9.90 < 9.11. \n",
      "This response is untrustworthy due to lack of consistency in possible responses from the model. Here's one inconsistent alternate response that the model considered (which may not be accurate either): \n",
      "9.11.\n"
     ]
    }
   ],
   "source": [
    "print(resp.additional_kwargs[\"explanation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a7616432-4bcb-4e66-bf30-35c64e927b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Haaga-Helia operates on five campuses.\n",
      "Trustworthiness score: 0.99\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How many campuses are there?Answer by including citations of document name and page numbers in the format [<document_name>, page <page_number>]\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "380ddf86-b0a1-4bd9-8c2c-5fd201e760c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing LLMs from https://en.wikipedia.org/wiki/Large_language_model ...\n",
      "Saved PDF for LLMs at C:/Users/h02317/Downloads/PDFs\\LLMs.pdf\n",
      "Parsing PDF for LLMs from C:/Users/h02317/Downloads/PDFs\\LLMs.pdf ...\n",
      "Started parsing the file under job_id 2b1ae611-0375-4998-a0f9-bab2e6f01d79\n",
      "..Saved combined markdown for LLMs to C:/Users/h02317/Downloads/PDFs\\markdown_docs\\LLMs.md\n",
      "Parsed 18 nodes from LLMs.\n",
      "Combined index and query engine created successfully!\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import VectorStoreIndex\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pdfkit\n",
    "from llama_index.readers.docling import DoclingReader\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.cleanlab import CleanlabTLM\n",
    "from typing import Dict, List, ClassVar\n",
    "from llama_index.core.instrumentation.events import BaseEvent\n",
    "from llama_index.core.instrumentation.event_handlers import BaseEventHandler\n",
    "from llama_index.core.instrumentation import get_dispatcher\n",
    "from llama_index.core.instrumentation.events.llm import LLMCompletionEndEvent\n",
    "import nest_asyncio\n",
    "import os\n",
    "from llama_index.core import PromptTemplate\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#############################\n",
    "# Event Handler for Trustworthiness Score\n",
    "#############################\n",
    "\n",
    "class GetTrustworthinessScore(BaseEventHandler):\n",
    "    events: ClassVar[List[BaseEvent]] = []\n",
    "    trustworthiness_score: float = 0.0\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"GetTrustworthinessScore\"\n",
    "\n",
    "    def handle(self, event: BaseEvent) -> Dict:\n",
    "        if isinstance(event, LLMCompletionEndEvent):\n",
    "            self.trustworthiness_score = event.response.additional_kwargs.get(\"trustworthiness_score\", 0.0)\n",
    "            self.events.append(event)\n",
    "        return {}\n",
    "\n",
    "def display_response(response):\n",
    "    response_str = response.response\n",
    "    trustworthiness_score = event_handler.trustworthiness_score\n",
    "    rsp = f\"\"\"\n",
    "    Response = {response_str}\n",
    "    \n",
    "    Trustworthiness score = {round(trustworthiness_score, 2)}\n",
    "    \n",
    "    \"\"\"\n",
    "    display(Markdown(rsp))\n",
    "\n",
    "#############################\n",
    "# LLM and Embedding Settings\n",
    "#############################\n",
    "\n",
    "options = {\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"max_tokens\": 512,\n",
    "    \"log\": [\"explanation\"]\n",
    "}\n",
    "\n",
    "llm = CleanlabTLM(api_key=\"CLEANLAB_API_KEY\", options=options)\n",
    "Settings.llm = llm\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "# Set up the dispatcher and register the event handler.\n",
    "root_dispatcher = get_dispatcher()\n",
    "event_handler = GetTrustworthinessScore()\n",
    "root_dispatcher.add_event_handler(event_handler)\n",
    "\n",
    "##########################################\n",
    "# PDF Generation from Multiple URLs\n",
    "##########################################\n",
    "\n",
    "# Configure wkhtmltopdf path\n",
    "wkhtml_path = r'C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe'\n",
    "config = pdfkit.configuration(wkhtmltopdf=wkhtml_path)\n",
    "\n",
    "# Define URLs and assign document names\n",
    "urls = {\n",
    "    \"LLMs\": \"https://en.wikipedia.org/wiki/Large_language_model\"\n",
    "}\n",
    "\n",
    "# Directory to save PDFs\n",
    "pdf_directory = \"PDFs\"\n",
    "os.makedirs(pdf_directory, exist_ok=True)\n",
    "\n",
    "pdf_paths = {}\n",
    "for doc_name, url in urls.items():\n",
    "    try:\n",
    "        print(f\"Processing {doc_name} from {url} ...\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        main_content = soup.find(\"div\", {\"id\": \"mw-content-text\"})\n",
    "        if main_content is None:\n",
    "            raise ValueError(\"Main content not found\")\n",
    "        # Replace protocol-relative URLs with absolute URLs\n",
    "        html_string = str(main_content).replace('src=\"//', 'src=\"https://').replace('href=\"//', 'href=\"https://')\n",
    "        pdf_file_path = os.path.join(pdf_directory, f\"{doc_name}.pdf\")\n",
    "        pdfkit.from_string(\n",
    "            html_string,\n",
    "            pdf_file_path,\n",
    "            options={'encoding': 'UTF-8', 'quiet': ''},\n",
    "            configuration=config\n",
    "        )\n",
    "        pdf_paths[doc_name] = pdf_file_path\n",
    "        print(f\"Saved PDF for {doc_name} at {pdf_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {doc_name}: {e}\")\n",
    "\n",
    "##########################################\n",
    "# Parse PDFs with LlamaParse and Inject Metadata\n",
    "##########################################\n",
    "\n",
    "# Define parsing instructions (if your parser supports it)\n",
    "parsing_instructions = \"\"\"Extract the document content in markdown.\n",
    "Split the document into nodes (for example, by page).\n",
    "Ensure each node has metadata for document name and page number.\"\"\"\n",
    "        \n",
    "# Create a LlamaParse instance (replace the API key below with your actual key)\n",
    "parser = LlamaParse(\n",
    "    api_key=\"llx-...\",   # Replace with your LLAMACLOUD key\n",
    "    parsing_instructions=parsing_instructions,\n",
    "    result_type=\"markdown\",\n",
    "    premium_mode=True,\n",
    "    max_timeout=600\n",
    ")\n",
    "\n",
    "# Directory to save combined Markdown files (one per PDF)\n",
    "output_md_dir = os.path.join(pdf_directory, \"markdown_docs\")\n",
    "os.makedirs(output_md_dir, exist_ok=True)\n",
    "\n",
    "# List to hold all updated nodes for indexing\n",
    "all_nodes = []\n",
    "\n",
    "for doc_name, pdf_path in pdf_paths.items():\n",
    "    try:\n",
    "        print(f\"Parsing PDF for {doc_name} from {pdf_path} ...\")\n",
    "        nodes = parser.load_data(pdf_path)  # Returns a list of nodes\n",
    "        updated_nodes = []\n",
    "        # Process each node: update metadata and inject citation header into the text.\n",
    "        for i, node in enumerate(nodes, start=1):\n",
    "            # Copy existing metadata (if any) and add our own keys.\n",
    "            new_metadata = dict(node.metadata) if node.metadata else {}\n",
    "            new_metadata[\"document_name\"] = doc_name\n",
    "            if \"page_number\" not in new_metadata:\n",
    "                new_metadata[\"page_number\"] = str(i)\n",
    "            # Build the citation header.\n",
    "            citation_header = f\"[{new_metadata['document_name']}, page {new_metadata['page_number']}]\\n\\n\"\n",
    "            # Prepend the citation header to the node's text.\n",
    "            updated_text = citation_header + node.text\n",
    "            new_node = node.__class__(text=updated_text, metadata=new_metadata)\n",
    "            updated_nodes.append(new_node)\n",
    "        # Save a single combined Markdown file for the document using the updated node texts.\n",
    "        combined_texts = [node.text for node in updated_nodes]\n",
    "        combined_md = \"\\n\\n---\\n\\n\".join(combined_texts)\n",
    "        md_filename = f\"{doc_name}.md\"\n",
    "        md_filepath = os.path.join(output_md_dir, md_filename)\n",
    "        with open(md_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(combined_md)\n",
    "        print(f\"Saved combined markdown for {doc_name} to {md_filepath}\")\n",
    "        # Add the updated nodes to the global list for indexing.\n",
    "        all_nodes.extend(updated_nodes)\n",
    "        print(f\"Parsed {len(updated_nodes)} nodes from {doc_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {doc_name}: {e}\")\n",
    "\n",
    "##########################################\n",
    "# Create Combined Index and Query Engine\n",
    "##########################################\n",
    "\n",
    "# Create a combined index from all nodes.\n",
    "index = VectorStoreIndex.from_documents(documents=all_nodes)\n",
    "\n",
    "# Define and create the PromptTemplate object\n",
    "prompt_template = PromptTemplate(\"\"\"\\\n",
    "You are an AI assistant with expertise in the subject matter.\n",
    "Answer the question using ONLY the provided context.\n",
    "Answer in well-formatted Markdown with bullets and sections wherever necessary.\n",
    "If the provided context does not support an answer, respond with \"I don't know.\"\n",
    "Include citations of document name and page numbers in the format [<document_name>, page <page_number>].\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Question:\n",
    "{query_str}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "# Create a query engine with the custom prompt.\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    llm=llm,\n",
    "    text_qa_template=prompt_template  \n",
    ")\n",
    "\n",
    "print(\"Combined index and query engine created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "10626586-d744-4c62-b9c7-917b7efdfbe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "    Response = The mixture of experts (MoE) approach is used when the largest large language models (LLMs) are too expensive to train and use directly. This approach allows for the training of models with a very large number of parameters, reaching up to 1 trillion, as pursued by Google researchers since 2017 [LLMs, page 4].\n",
       "    \n",
       "    Trustworthiness score =  0.98\n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"When is mixture of experts approach used? \")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "4529f96e-3519-4fd5-a800-186f34799b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "    Response = - **Performance and Cost-Effectiveness**:\n",
       "  - The DeepSeek-R1 model, released by the Chinese company DeepSeek, achieved comparable performance to OpenAI's o1 model. This indicates that DeepSeek-R1 is competitive in terms of its capabilities in reasoning tasks.\n",
       "  - DeepSeek-R1 is noted for being significantly more cost-effective to operate compared to OpenAI's models. This suggests that it may require less computational resources or be optimized for efficiency in some way [LLMs, page 7].\n",
       "\n",
       "- **Openness and Accessibility**:\n",
       "  - Unlike OpenAI's proprietary models, DeepSeek-R1 is an open-weight model. This means that its weights are accessible to researchers, allowing them to study and build upon the algorithm. This openness can foster more collaborative research and development in the AI community.\n",
       "  - However, it is important to note that while the model weights are open, the training data for DeepSeek-R1 remains private, which may limit some aspects of transparency and reproducibility [LLMs, page 7].\n",
       "\n",
       "- **Domain Capabilities**:\n",
       "  - Both DeepSeek-R1 and OpenAI's reasoning models, such as o1, have shown superior capabilities in domains requiring structured logical thinking, such as mathematics, scientific research, and computer programming. This highlights their effectiveness in complex reasoning tasks [LLMs, page 7].\n",
       "\n",
       "Overall, DeepSeek-R1 offers a competitive and cost-effective alternative to OpenAI's models, with the added benefit of open weights for research purposes, although it maintains some limitations in data transparency.\n",
       "    \n",
       "    Trustworthiness score =  0.96\n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"How do you compare Deepseek model with OpenAI's models? \")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "8b325352-9aef-468a-a590-5295aad39602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer = 9.11 is bigger than 9.9.\n",
      "Trustworthiness score = 0.41811348835461787\n",
      "Explanation: This response is untrustworthy due to lack of consistency in possible responses from the model. Here's one inconsistent alternate response that the model considered (which may not be accurate either): \n",
      "9.9 is bigger.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Getting the trustworthiness score of an LLM response and its explanation\"\"\"\n",
    "\n",
    "from cleanlab_studio import Studio\n",
    "import streamlit as st\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "import os\n",
    "GROQ_API_KEY = \"...\" #get GROQ API key from https://groq.com/\n",
    "\n",
    "# Initialize the Groq Llama Instant model\n",
    "groq_llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0.5)\n",
    "# Ask a simple question\n",
    "question = \"Which one of 9.11 and 9.9 is bigger?\"\n",
    "# Construct a simple prompt\n",
    "prompt = (\n",
    "    f\"You are a helpful assistant.\\n\"\n",
    "    f\"Answer the following question clearly and concisely:\\n\\n\"\n",
    "    f\"Question: {question}\\n\"\n",
    "    f\"Answer:\"\n",
    ")\n",
    "# Get the response from the model\n",
    "response = groq_llm.invoke(prompt)\n",
    "# Print the response\n",
    "print(f\"Answer = {response.content.strip()}\")\n",
    "\n",
    "studio = Studio(\"CLEANLAB_API_KEY\")  # Get your free API key from: https://tlm.cleanlab.ai/\n",
    "cleanlab_tlm = studio.TLM(options={\"log\": [\"explanation\"]})  # See Advanced Tutorial for optional TLM configurations to get better/faster results\n",
    "prompt = \"Which one of 9.11 and 9.9 is bigger?\"\n",
    "output = cleanlab_tlm.get_trustworthiness_score(prompt, response=response.content.strip())\n",
    "\n",
    "print(f\"Trustworthiness score = {output[\"trustworthiness_score\"]}\")\n",
    "print(f'Explanation: {output[\"log\"][\"explanation\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "8704fa90-c265-4363-8001-5e250289765a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h02317\\AppData\\Local\\Temp\\ipykernel_35044\\2854823614.py:3: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, Markdown\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Model: gpt-4o\n",
       "\n",
       "**Response:** 9.11 is bigger than 9.9. When comparing decimal numbers, you start by comparing the digits from left to right. Both numbers have the same whole number part (9), but in the tenths place, 9.11 has a 1 while 9.9 has a 9. Since 9.9 can be thought of as 9.90, comparing the hundredths place shows that 1 is greater than 0, making 9.11 the larger number.\n",
       "\n",
       "**Trustworthiness Score:** 0.5913138800517392\n",
       "\n",
       "**Explanation:** incorrect. When comparing decimal numbers, you start by comparing the digits from left to right. Both numbers have the same whole number part (9). In the tenths place, 9.11 has a 1, and 9.9 has a 9. However, the comparison should be made by considering 9.9 as 9.90 to align the decimal places. In this case, 9.90 is greater than 9.11 because 9 in the tenths place is greater than 1 in the tenths place. Therefore, 9.9 (or 9.90) is actually larger than 9.11. \n",
       "This response is untrustworthy due to lack of consistency in possible responses from the model. Here's one inconsistent alternate response that the model considered (which may not be accurate either): \n",
       "9.9 is bigger.\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Model: claude-3.5-sonnet-v2\n",
       "\n",
       "**Response:** 9.11 is bigger than 9.9\n",
       "\n",
       "To compare decimal numbers, we can look at each decimal place from left to right. In this case:\n",
       "- Both numbers have 9 in the ones place\n",
       "- In the tenths place (first decimal place), 9.11 has 1 and 9.9 has 9\n",
       "- Since 1 is less than 9, we need to look at the hundredths place\n",
       "- 9.11 has 1 in the hundredths place, while 9.9 is the same as 9.90\n",
       "- Therefore, 9.11 > 9.90 (or 9.9)\n",
       "\n",
       "So, 9.11 is the bigger number.\n",
       "\n",
       "**Trustworthiness Score:** 0.5542884316430033\n",
       "\n",
       "**Explanation:** Let me solve this step by step:\n",
       "\n",
       "1) When comparing decimal numbers, we align the decimal points and compare digits from left to right.\n",
       "\n",
       "2) Let's write both numbers:\n",
       "   9.11\n",
       "   9.90 (9.9 = 9.90)\n",
       "\n",
       "3) Starting from the left:\n",
       "   - Ones place: both have 9, so they're equal\n",
       "   - Tenths place (first decimal): \n",
       "     * 9.11 has 1\n",
       "     * 9.9 has 9\n",
       "   - Since 9 is greater than 1 in the tenths place, we don't need to look further\n",
       "\n",
       "4) The AI's answer claims 9.11 is bigger than 9.9, which is incorrect.\n",
       "   9.9 = 9.90 > 9.11\n",
       "\n",
       "5) The AI's explanation about needing to look at the hundredths place is wrong. Once we find a difference in a decimal place, we can stop comparing.\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Comparing the answer, trustworthiness scores, and explanations of two LLMs for the same prompt\"\"\"\n",
    "\n",
    "from cleanlab_studio import Studio\n",
    "import markdown\n",
    "from IPython.core.display import display, Markdown\n",
    "\n",
    "# Initialize the Cleanlab Studio with API key\n",
    "studio = Studio(\"CLEANLAB_API_KEY\")  # Replace with your actual API key\n",
    "\n",
    "# List of models to evaluate\n",
    "models = [\"gpt-4o\", \"claude-3.5-sonnet-v2\"]\n",
    "\n",
    "# Define the prompt\n",
    "prompt_text = \"Which one of 9.11 and 9.9 is bigger?\"\n",
    "\n",
    "# Loop through each model and evaluate\n",
    "for model in models:\n",
    "    tlm = studio.TLM(options={\"log\": [\"explanation\"], \"model\": model})\n",
    "    out = tlm.prompt(prompt_text)\n",
    "    \n",
    "    md_content = f\"\"\"\n",
    "## Model: {model}\n",
    "\n",
    "**Response:** {out['response']}\n",
    "\n",
    "**Trustworthiness Score:** {out['trustworthiness_score']}\n",
    "\n",
    "**Explanation:** {out['log']['explanation']}\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "    display(Markdown(md_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8e9da2ae-cee1-48a0-b98b-31f70e97414c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Model: claude-3.5-sonnet-v2\n",
       "\n",
       "**Response:** <think>\n",
       "First, I will compare the whole number parts of both numbers. Both 9.11 and 9.9 have the same whole number, which is 9.\n",
       "\n",
       "Next, I will compare the decimal parts. For 9.11, the decimal part is 0.11, and for 9.9, it is 0.9.\n",
       "\n",
       "Since 0.9 is greater than 0.11, the decimal part of 9.9 is larger.\n",
       "\n",
       "Therefore, 9.9 is the bigger number.\n",
       "</think>\n",
       "\n",
       "To determine which number is larger between **9.11** and **9.9**, let's compare them step by step.\n",
       "\n",
       "1. **Compare the Whole Number Parts:**\n",
       "   - Both numbers have the same whole number part, which is **9**.\n",
       "\n",
       "2. **Compare the Decimal Parts:**\n",
       "   - For **9.11**, the decimal part is **0.11**.\n",
       "   - For **9.9**, the decimal part is **0.9**.\n",
       "\n",
       "3. **Determine Which Decimal is Larger:**\n",
       "   - **0.9** is greater than **0.11** because **0.9** is equivalent to **0.90**, and **0.90 > 0.11**.\n",
       "\n",
       "Since the decimal part of **9.9** is larger than that of **9.11**, **9.9** is the bigger number.\n",
       "\n",
       "\\[\n",
       "\\boxed{9.9}\n",
       "\\]\n",
       "\n",
       "**Trustworthiness Score:** 0.9833656002275584\n",
       "\n",
       "**Explanation:** Did not find a reason to doubt trustworthiness.\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "import os\n",
    "GROQ_API_KEY = \"...\" #get GROQ API key from https://groq.com/\n",
    "\n",
    "# Initialize the Groq Llama Instant model\n",
    "groq_llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\", temperature=0.5)\n",
    "prompt = \"Which one of 9.11 and 9.9 is bigger?\"\n",
    "# Get the response from the model\n",
    "response = groq_llm.invoke(prompt)\n",
    "#Initialize Cleanlab's studio\n",
    "studio = Studio(\"CLEANLAB_API_KEY\")  \n",
    "cleanlab_tlm = studio.TLM(options={\"log\": [\"explanation\"]})  #for explanations\n",
    "#Get the output containing trustworthiness score and explanation\n",
    "output = cleanlab_tlm.get_trustworthiness_score(prompt, response=response.content.strip())\n",
    "\n",
    "md_content = f\"\"\"\n",
    "## Model: {model}\n",
    "\n",
    "**Response:** {response.content.strip()}\n",
    "\n",
    "**Trustworthiness Score:** {output['trustworthiness_score']}\n",
    "\n",
    "**Explanation:** {output['log']['explanation']}\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "display(Markdown(md_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1339274-25e2-4e10-83c0-ead4b87b288b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
